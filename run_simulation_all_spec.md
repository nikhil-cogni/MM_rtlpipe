# Specification for run_simulation_all.sh

## Overview

`run_simulation_all.sh` is a shell script designed to perform systematic testing of a collection of arithmetic modules in SystemVerilog using the Verilator simulator. The script tests all 25 arithmetic modules defined in `arithmetic_modules.sv` with configurable data widths and pipeline stages, collects execution metrics, and generates HTML reports of the test results.

## Context in Repository

This script is part of a hardware design and verification repository containing:

1. `arithmetic_modules.sv`: Contains 25 different arithmetic modules, each implementing various combinational logic operations with parameterizable bit widths
2. `pipelined_arithmetic.sv`: Contains wrapper modules to insert pipeline stages either before or after the arithmetic operations
3. `tb_pipelined_arithmetic.sv`: Testbench that verifies pipeline equivalence by comparing outputs of different pipeline arrangements

The purpose of this testing framework is to verify that the output of pipelined arithmetic operations matches regardless of whether the pipeline stages are inserted before or after the combinational logic.

## Detailed Functionality

### Configuration Parameters

- `WIDTH`: Data width for the arithmetic modules (default: 16 bits)
- `PIPE_STAGES`: Number of pipeline stages to insert (default: 3 stages)
- `TOTAL_MODULES`: Number of modules to test (default: 25)

### Data Structures

- `results_status`: Array storing pass/fail status for each module
- `results_time`: Array storing execution time for each module
- `total_passed`: Counter for passed tests
- `total_failed`: Counter for failed tests
- `LOG_DIR`: Directory where test logs are stored

### Workflow

1. **Setup and Initialization**
   - Create a log directory to store individual module test results
   - Initialize arrays to track test results and timing
   - Print a header with test configuration details

2. **Module Testing Loop**
   - For each module (1 through 25):
     - Create a unique log file for this module's test results
     - Record starting time for metrics
     - Run Verilator compilation with appropriate parameters:
       - Module ID selection
       - WIDTH configuration
       - PIPE_STAGES configuration
     - Check if compilation succeeded:
       - If failed, mark test as failed and continue to next module
     - Run the simulation executable
     - Record execution time
     - Check for "TEST PASSED" in the log file:
       - If found, mark test as passed
       - If not found, mark test as failed
     - Update pass/fail counters

3. **Results Reporting**
   - Calculate total test duration
   - Print a summary table showing:
     - Status of each module (PASSED/FAILED)
     - Execution time for each module
   - Print overall test statistics:
     - Total passed tests
     - Total failed tests
     - Total execution time

4. **HTML Report Generation**
   - Create an HTML report with:
     - Test configuration details (WIDTH, PIPE_STAGES)
     - Table of results for all modules
     - Color-coded status indicators (green for pass, red for fail)
     - Summary statistics
     - Visual success/failure indicator

### Command-Line Parameters

The basic version of the script doesn't accept command-line parameters, using fixed configuration values.

### Verilator Command Details

For each module, the script runs Verilator with these options:
- `--binary`: Generate a binary executable directly
- `--timing`: Include timing simulation
- `--assert`: Enable SystemVerilog assertions
- `--autoflush`: Automatically flush output streams
- `-j 2`: Use 2 cores for compilation
- `-sv`: Enable SystemVerilog features
- Various warning suppressions: `-Wno-CASEINCOMPLETE`, `-Wno-REALCVT`, etc.
- `--trace-structs`, `--trace-params`, `--trace-fst`: Generate FST waveforms
- `-top tb_pipelined_arithmetic`: Set top module
- `-o sim_${module}.exe`: Name the output executable
- Define preprocessing macros: `+define+SIMULATION`, `+define+MODULE_ID=${module}`, etc.

## Output Files

1. **Log Files**
   - Individual log file for each module: `${LOG_DIR}/module_${module}.log`
   - Contains Verilator compilation output and simulation results

2. **HTML Report**
   - `test_report.html`: Summary of all test results
   - Features:
     - Formatted HTML table with test results
     - Color-coding (green for passed, red for failed)
     - Execution timing information
     - Overall success/failure indicator

3. **Simulation Output**
   - `dumpfile.fst`: Waveform file (if generated by the testbench)

## Test Pass/Fail Criteria

A test is considered passed if the string "TEST PASSED" is found in the module's log file, which indicates that the testbench verified identical behavior between the two pipeline arrangements.

## Error Handling

- Compilation errors are detected by checking the exit code of the Verilator command
- Simulation errors are detected by searching for "TEST PASSED" in the log file
- Failed modules are reported in the summary but don't stop the testing of other modules

## Performance Metrics

The script measures and reports:
- Individual execution time for each module
- Total execution time for the entire test suite

## Technical Implementation Notes

- Uses bash shell features like arrays, process substitution, and command substitution
- Relies on standard UNIX tools like grep, awk, and date
- Generates HTML using heredocs (EOF-delimited text blocks)
- Status is tracked numerically for easy aggregation of results

## Implementation Considerations for Python Port

When porting this script to Python, consider:

1. **Process Execution**:
   - Use `subprocess` module to run Verilator commands
   - Consider using `subprocess.run()` with `capture_output=True` to capture stdout/stderr

2. **File Handling**:
   - Use Python's file handling capabilities for log files and HTML report generation
   - Consider using template libraries like Jinja2 for HTML report generation

3. **Parallelization**:
   - Python's `concurrent.futures` module could be used to parallelize module testing

4. **Error Handling**:
   - Implement more robust error handling using Python's exception mechanisms
   - Add more detailed error reporting

5. **Command-Line Interface**:
   - Use `argparse` for a more sophisticated command-line interface
   - Add options for specifying WIDTH and PIPE_STAGES from the command line

This specification provides a comprehensive overview of the `run_simulation_all.sh` script, detailing its purpose, functionality, inputs, outputs, and implementation details, which should be sufficient for porting to another language such as Python.
